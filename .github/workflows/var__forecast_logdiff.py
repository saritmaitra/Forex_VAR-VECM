# -*- coding: utf-8 -*-
"""VAR_ FORECAST_logdiff.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QCcs_Y-ElSL-8NjNA2mej5M-LYp7ywxa
"""

!pip install pyforest
from pyforest import *
import datetime, pickle, copy, warnings
import math
from pandas import DataFrame, merge, concat
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)

"""# 1. Data loading and initial preprocessing:"""

from google.colab import files
uploaded = files.upload()

print('view data...')
df = pd.read_csv("AL_final_data.csv")
df.head()

print('dropping unnecessary column...')
df.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)
df.tail()

print('setting datetime index....')
df = df.set_index('timestamp')
df.index = pd.to_datetime(df.index)
df.info()

print('dropping duplicates (if any)...')
df.drop_duplicates(keep=False)
df.info()

df = df.sort_index(ascending=True)

# Plot
plt.style.use('dark_background')
fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))

for i, ax in enumerate(axes.flatten()):
    dataset = df[df.columns[i]]
    ax.plot(dataset, color='red', linewidth=1)
    # Decorations
    ax.set_title(df.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines['top'].set_alpha(0)
    ax.tick_params(labelsize=6)
    plt.tight_layout();

"""We had seen earlier during exploratory analysis and more so, Visually it can be assumed that, none of the original series are stationary and have Brownian motion movement.

# 1 Multicolinearity
### 1.1 Pearson Correlation:
"""

col = df.copy()
print('correlation matrix (Original series):'); print()
print(col.corr())

plt.style.use('dark_background')
plt.figure(figsize = (14,6)); print()
pearsoncorr = col.corr(method='pearson')
sns.heatmap(pearsoncorr, xticklabels=pearsoncorr.columns,
            yticklabels=pearsoncorr.columns,
            cmap='RdBu_r',annot=True,linewidth=0.5)
plt.show()

"""we have seen this earlier during exploratory analysis that, the largest observed correlations (in absolute value) are:
- 0.90 between ES & NQ;
- 0.68 between the EURUSD & ES; 
- 0.90 between NQ & ES;
- 0.72 between USDJPY & NQ; 
- 0.66 between USDJPY & ES 
- 0.68 between GBPUSD & GC (negative significant correlation)

We have considered >6 as significant correlation.

We shall recheck the correlation later on transformed dataset.

## 1.1 Histograms and Density plots:
"""

plt.style.use('dark_background')
ax = df.eurusd.plot(kind='kde', secondary_y=True)
df.eurusd.plot(kind='hist', bins=50)
plt.title('EURUSD')
plt.show()

plt.style.use('dark_background')
ax = df.gbpusd.plot(kind='kde', secondary_y=True)
df.gbpusd.plot(kind='hist', bins=50)
plt.title('GBPUSD')
plt.show()

plt.style.use('dark_background')
ax = df.usdjpy.plot(kind='kde', secondary_y=True)
df.usdjpy.plot(kind='hist', bins=50)
plt.title('USDJPY')
plt.show()

plt.style.use('dark_background')
ax = df.nq.plot(kind='kde', secondary_y=True)
df.nq.plot(kind='hist', bins=50)
plt.title('NQ')
plt.show()

plt.style.use('dark_background')
ax = df.gc.plot(kind='kde', secondary_y=True)
df.gc.plot(kind='hist', bins=50)
plt.title('gc')
plt.show()

"""## 1.2 Normality test:"""

import statsmodels.stats.api as sms
from statsmodels.compat import lzip

"""
General guideline for skewness:
if the number > +1 or < ‚Äì1, this is an indication of a substantially skewed distribution. 
For kurtosis:
if the number > +1, the distribution is too peaked. 
Likewise, a kurtosis of < ‚Äì1 indicates a distribution that is too flat. 
Distributions exhibiting skewness and/or kurtosis that exceed these guidelines are considered nonnormal.
Ref: (Hair et al., 2017, p. 61).
"""
name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']
test = sms.jarque_bera(df)
lzip(name, test)

"""- skewness is positive in 4 series (ES, EURUSD, GBPUSD, GC); the data are skewed right, meaning that the right tail of the distribution is longer than the left. 
- skewness is negative in 2 series (NQ, USDJPY); the data are skewed left, meaning that the left tail of the distribution is longer than the right.

here are the general assumptions:
- skewness is less than ‚àí1 or greater than +1, the distribution is highly skewed.
- skewness is between ‚àí1 and ‚àí¬Ω or between +¬Ω and +1, the distribution is moderately skewed.
- kewness is between ‚àí¬Ω and +¬Ω, the distribution is approximately symmetric.

Going by skew values (0.10577135,  0.62682264,  0.46015031,  0.37613819, -0.53754677, -0.28981428), we can assume that our data distribution is approximately symmetric.
"""

# mean, std and IQR values
df.describe()

# spliting train/test
#nobs = 10 # keeping 10 mins observations aside for validation
#train_var, test_var = df[0:- nobs], df[- nobs:]

#train_trans = np.log(train_var).diff().dropna()
#print('Description of diff data:'); print(train_trans.describe());

#train_trans = np.log(train_var/train_var.shift(1)).dropna()
#train_trans

"""## 1.1 Spliting train/test


"""

# spliting train/test
nobs = 10 # keeping 10 mins observations aside for validation
train_var, test_var = df[0:- nobs], df[- nobs:]

# Check size
print(train_var.shape, test_var.shape); print()

"""
stationary time series means a constant mean and variance over time, which makes it easy for predicting values.
"""

# transfoming training data set
train_log = np.log(train_var) # log transformation

"""
Performed log transformation to normalize skewed variables. Our varibales here having non-linear relationships.
So, while modeling, the chances of producing errors may also be skewed negatively. 
We want to produce the smallest error possible when making prediction, moreover, we should not be overfitting the model. 
"""

train_trans = train_log.diff().dropna() # 1st order differencing

"""
finding the differences between consecutive values of the series
"""
print('Description of diff data:'); print(train_trans.describe());

plt.style.use('dark_background')
fig, ax1 = plt.subplots(figsize = (15,6))

color = 'tab:red'
ax1.set_xlabel('Timestamp (minute frequency)')
ax1.set_ylabel('Natural Log of Gold Prices', color = color)
ax1.plot(train_log.gc, color=color)
ax1.tick_params(axis = 'y', labelcolor = color)
ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

color = 'tab:blue'
ax2.set_ylabel('Natural Log of EURUSD', color = color)  
ax2.plot(train_log.eurusd, color = color)
ax2.tick_params(axis = 'y', labelcolor = color)

plt.title('Natural Log of Gold and EURUSD Exchange Rates')
fig.tight_layout()  
plt.show()

"""Gold & EURUSD seem appear inversely related over time. When one goes up, the other goes down.

We need to test structural break i.e. when series abruptly changes at a point in time. This change could involve a change in mean or a change in the other parameters of the process that produce the series.

If we can detect when the structure of the series changes can help help us to determine when and whether there is a significant change in our data.

### 1.1.1 Multi-collinearity check on differenced data:
"""

# rechecking correlation on transformed data
col = train_trans.copy()
print('correlation matrix (differenced data):')
print(col.corr())
plt.figure(figsize = (14,6)); print()
pearsoncorr = col.corr(method='pearson')
sns.heatmap(pearsoncorr, xticklabels=pearsoncorr.columns,
            yticklabels=pearsoncorr.columns,
            cmap='RdBu_r',annot=True,linewidth=0.5)
plt.show()

"""- The largest obseved +ve correlation is 0.88 between GC and ES. Therefore, we will keep one and drop other.
- largest observerd -ve correlaion is -0.024 between EURUSD and USDJPY which is small enough to ignore.
 
"""

# dropping ES from transformed dataset
#train_trans.drop(columns = ['es'], axis=1, inplace=True)
#print(train_trans.corr()) # rechecking correlation

"""Now the observed correlations are quite small and that they can reasonably be ignored.
## 1.2 Lag order selection:

### 1.2.1 Checking required lags through ACF/PACF:
"""

import statsmodels.tsa.api as smt
from pandas.testing import assert_frame_equal

acf,q,pval = smt.acf(train_trans['eurusd'],nlags=12,qstat=True, fft=False)
pacf = smt.pacf(train_trans['eurusd'],nlags=12)
correlogram = pd.DataFrame({'acf':acf[1:], 'pacf':pacf[1:], 'Ljung-Box Q stat':q,'p-val':pval})
print(correlogram)

from statsmodels.tsa.stattools import acf, pacf
import pandas.util.testing as tm

lag_acf = acf(train_trans.eurusd, nlags=36, fft=False)
lag_pacf = pacf(train_trans.eurusd, method = 'ols', nlags=36) 

plt.figure(figsize=(14,5))
plt.style.use('dark_background')
#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.eurusd)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.eurusd)),linestyle='--',color='gray')
plt.title('ACF:EURUSD')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.eurusd)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.eurusd)),linestyle='--',color='gray')
plt.title('PACF:EURUSD')
plt.tight_layout()

"""It is evident that the series is quite persistent. The autocorrelation function dies away rather quickly. The numerical values of the autocorrelation and partial autocorrelation coefficients at lags 1--12 are given in the second and forth columns of the output, with the lag length given in the first column.

The first acf coefficient is highly significant, the joint test statistic presented in column 3 rejects the null hypothesis of no autocorrelation at the 1{%} level for all numbers of lags considered.

"""

import statsmodels.tsa.api as smt
acf,q,pval = smt.acf(train_trans['gbpusd'],nlags=12,qstat=True, fft=False)
pacf = smt.pacf(train_trans['gbpusd'],nlags=12)
correlogram = pd.DataFrame({'acf':acf[1:], 'pacf':pacf[1:], 'Ljung-Box Q stat':q,'p-val':pval})
print(correlogram)

from statsmodels.tsa.stattools import acf, pacf
import pandas.util.testing as tm

lag_acf = acf(train_trans.gbpusd, nlags=36, fft=False)
lag_pacf = pacf(train_trans.gbpusd, method = 'ols', nlags=36) 

plt.figure(figsize=(14,5))
plt.style.use('dark_background')
#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.gbpusd)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.gbpusd)),linestyle='--',color='gray')
plt.title('ACF:GBPUSD')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.gbpusd)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.gbpusd)),linestyle='--',color='gray')
plt.title('PACF:GBPUSD')
plt.tight_layout()

import statsmodels.tsa.api as smt
acf,q,pval = smt.acf(train_trans['usdjpy'],nlags=12,qstat=True, fft=False)
pacf = smt.pacf(train_trans['usdjpy'],nlags=12)
correlogram = pd.DataFrame({'acf':acf[1:], 'pacf':pacf[1:], 'Ljung-Box Q stat':q,'p-val':pval})
print(correlogram)

from statsmodels.tsa.stattools import acf, pacf
import pandas.util.testing as tm

lag_acf = acf(train_trans.usdjpy, nlags=36, fft=False)
lag_pacf = pacf(train_trans.usdjpy, method = 'ols', nlags=36) 

plt.figure(figsize=(14,5))
plt.style.use('dark_background')
#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.usdjpy)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.usdjpy)),linestyle='--',color='gray')
plt.title('ACF:USDJPY')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.usdjpy)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.usdjpy)),linestyle='--',color='gray')
plt.title('PACF:USDJPY')
plt.tight_layout()

import statsmodels.tsa.api as smt
acf,q,pval = smt.acf(train_trans.gc,nlags=12,qstat=True, fft=False)
pacf = smt.pacf(train_trans.gc,nlags=12)
correlogram = pd.DataFrame({'acf':acf[1:], 'pacf':pacf[1:], 'Ljung-Box Q stat':q,'p-val':pval})
print(correlogram)

from statsmodels.tsa.stattools import acf, pacf
import pandas.util.testing as tm

lag_acf = acf(train_trans.gc, nlags=36, fft=False)
lag_pacf = pacf(train_trans.gc, method = 'ols', nlags=36) 

plt.figure(figsize=(14,5))
plt.style.use('dark_background')
#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.gc)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.gc)),linestyle='--',color='gray')
plt.title('ACF:GC')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.gc)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.gc)),linestyle='--',color='gray')
plt.title('PACF:GC')
plt.tight_layout()

import statsmodels.tsa.api as smt
acf,q,pval = smt.acf(train_trans.nq,nlags=12,qstat=True, fft=False)
pacf = smt.pacf(train_trans.nq,nlags=12)
correlogram = pd.DataFrame({'acf':acf[1:], 'pacf':pacf[1:], 'Ljung-Box Q stat':q,'p-val':pval})
print(correlogram)

from statsmodels.tsa.stattools import acf, pacf
import pandas.util.testing as tm

lag_acf = acf(train_trans.nq, nlags=36, fft=False)
lag_pacf = pacf(train_trans.nq, method = 'ols', nlags=36) 

plt.figure(figsize=(14,5))
plt.style.use('dark_background')
#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.nq)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.nq)),linestyle='--',color='gray')
plt.title('ACF:NQ')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(train_trans.nq)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(train_trans.nq)),linestyle='--',color='gray')
plt.title('PACF:NQ')
plt.tight_layout()

"""We can conclude that all the series are correlated with itself shifted by 2 minutes.


### 1.2.2 Checking required lags through iteratively fitting:

"""

from statsmodels.tsa.api import VAR
pd.options.display.float_format = "{:.2f}".format
model = VAR(train_trans.values)
"""
construct of a VAR model and enter the variables to determine
the appropriate lag length
"""
res_aic = []
for i in range (1,15):
    res = model.fit(i)
    print('Lag Order =', i)
    print('AIC : ', res.aic)
    print('BIC : ', res.bic)
    print('FPE : ', res.fpe)
    print('HQIC: ', res.hqic, '\n')
    print(res_aic.append(res.aic)); print()

import seaborn as sns
sns.set()
plt.plot(list(np.arange(1,15,1)), res_aic)
plt.xlabel("Order")
plt.ylabel("AIC")
plt.show()

"""From the plot, the lowest AIC score is achieved at the order of 11. Hence, we select the 11 as the optimal order of the VAR model.
- BIC drops to lowest at lag 3
- FPE drops to lowest at lag 8
- HQIC drops to lowest at lag 4

We shall try with lag 8 and lag3 to check which models proves better output.

### 1.2.3 Checking throgh VAR order selection:
"""

x = model.select_order(maxlags=40)
print(x.summary())

# Visualization
AIC = {}
best_aic, best_order = np.inf, 0
for i in range(1,50):
    model = VAR(endog = train_trans.values).fit(maxlags=i)
    
    """
    construct of a VAR model and enter the variables to determine the appropriate lag length
    """
    AIC[i] = model.aic
    
    if AIC[i] < best_aic:
        best_aic = AIC[i]
        best_order = i
        
print('BEST ORDER', best_order, 'BEST AIC:', best_aic)

# Plot AICs 
plt.style.use('dark_background')
plt.figure(figsize=(14,5))
plt.plot(range(len(AIC)), list(AIC.values()))
plt.plot([best_order-1], [best_aic], marker='o', markersize=8, color="red")
plt.xticks(range(len(AIC)), range(1,50))
plt.xlabel('lags'); plt.ylabel('AIC')
np.set_printoptions(False)

"""- This gives AIC & FPE as 31, BIC & HQIC as 11
- We have few lag values, let us try with 2 lags.

# **2 VAR model fitting with lags 2:**
"""

import statsmodels.tsa.api as smt
from statsmodels.tsa.api import VAR

# pass "1min" frequency
train_trans.index = pd.DatetimeIndex(train_trans.index).to_period('1min')

# fitting VAR model with associated freq & lags
model_l_2 = VAR(endog = train_trans).fit(maxlags = 2)
model_l_2.summary()

"""- Top of the table shows the information for the model as a whole, including values of the information criteria, 
- Further down we shows the coefficient estimates and goodness-of-fit measures for each of the equations separately. 
- Each regression equation is separated by a horizontal line.

## 2.1 Normality test of residuals:
If the residuals follow a normal distribution we expect a histogram of the residuals to be bell-shaped (with no outliers).
"""

residuals = model_l_2.resid
print(residuals.shape)
residuals.index = (residuals.index).to_timestamp()

from statsmodels.graphics.api import qqplot
fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(111)
fig = qqplot(residuals, line='q', ax=ax, fit=True)

# imports the normality test from scipy.stats
from scipy.stats import normaltest
# performs the normality test on the residuals 
print(normaltest(residuals))

"""The test rejects the hypothesis of normality (p-value < 0.05)


"""

import statsmodels.stats.api as sms
from statsmodels.compat import lzip

"""### 2.1.1 Histogram plot:"""

#plt.figure(figsize = (10,5))
#residuals = model_l_2.resid
#print(residuals.shape)
#plt.figure(1)
#plt.hist(residuals, bins=50, edgecolor='black',linewidth=1.2)
#plt.xlabel('Residuals'); plt.ylabel('Density');
#plt.show()

"""However, looking at the histogram plot, we could investigate the distribution of the residuals whether resembles a bell-shape, any outliers which might lead to a considerable negative skewness. 

With the increase the number of bins or lower the width of the bins in order to obtain a more differentiated histogram.

Unfortunately we could not perform this test because of hardware issue.

### 2.1.2 Jarque bera test:
"""

name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']
test = sms.jarque_bera(residuals)
lzip(name, test)

"""- Negative values for the skewness indicate data that are skewed left. means the left tail is long relative to the right tail.

- Kurtosis with values > 3 means data exceeding the tails of the normal distribution.

Non-normality of residuals mean that these inferences could be wrong; however, our sample size is large enough that we need be less concerned than we would with a smaller sample.

### 2.1.3 Durbin Watson Statistic:

DW test was performed with the assumption that the errors are independent. Value of 2, or close to it, is ideal. The statistical value ranges between 0-4 where a value closer to 0 is more evidence for positive serial correlation and a value closer to 4 is more evidence for negative serial correlation.

If there is any correlation left in the residuals, then, there is some pattern in the time series that is still left to be explained by the model. In that case, the typical course of action is to either increase the order of the model or induce more predictors into the system or look for a different algorithm to model the time series. So, checking for serial correlation is to ensure that the model is sufficiently able to explain the variances and patterns in the time series.
"""

from statsmodels.stats.stattools import durbin_watson
out = durbin_watson(model_l_2.resid)

for col, val in zip(train_var.columns, out):
    print((col), ':', round(val, 2))

"""This is to determine the existence of multi-colinearity where variables are highly correlated. The statistic is a values between 0-4 and value 2 is idea, which means there is ni auto-correlation among the variables in the model.

The serial correlation looks alright.

### 2.1.4 Residuals plot
"""

# plot residual errors

res = DataFrame(residuals)
res.plot(figsize = (15,5))
plt.show()

res.plot(kind='kde', figsize = (15,5))
plt.show()

print(res.describe())

"""- line plot of the residual errors suggesting that model has captured the information quite well.
- density plot of the residual error values suggesting the errors are Gaussian, centered on zero.
- The distribution of the residual errors is displayed. The results show that there is no bias in the prediction (zero mean in the residuals).
"""

y_fitted = model_l_2.fittedvalues
plt.figure(figsize = (15,5))
plt.plot(residuals.values, label='resid')
plt.plot(y_fitted.values, label='linear prediction')
plt.xlabel('lag')
plt.xticks(rotation=45)
plt.ylabel('Residuals')
plt.title('Residuals plot = VAR lag 2')
plt.grid(True)
plt.legend()
plt.show()

"""Despite failing the normality tests, the mean of the residuals are essentially 0. """

# residuals sum of square (RSS)
(residuals**2).sum()

residuals.tail

"""### 2.1.5 Tests for structural breaks:
Brown, Durban, and Evans (1975) proposed CUMSUM test
"""

# unexpected change over time in the parameters of regression models
name = ['test statistic', 'pval', 'crit']
test = sms.breaks_cusumolsresid(residuals.values, ddof = model_l_2.df_model)
"""
greater the CUSUM test statistic, the greater the forecast error, 
and the greater the statistical evidence in favor of parameter instability.
"""
lzip(name, test)

"""- Null hypothesis (H0) = no structural break 
- The test statistic (0.68) and the corresponding p-value (0.73 > 0.05) suggest that we cannot reject H0 and our model does not have a structural break for any possible break date in the sample.
- we can conclude that our series is not abruptly changed at a point in time considering our historical data

***Ref: Ploberger, Werner, and Walter Kramer. ‚ÄúThe Cusum Test with Ols Residuals.‚Äù***
"""

# residuals statistic
residuals.describe()

plt.figure(figsize = (15,5))
plt.title('IQR plot for all variables')
sns.boxplot(x = residuals.values)
plt.show()

#import statsmodels.api as sm
#sm.qqplot(residuals,fit=True,line='45')
#plt.show()

# outlier values
residuals.nsmallest(n= 7, columns = residuals.columns)

# creating dummy variables with outliers

#train_trans['dum_1'] = np.where(train_trans.index == '2020-05-29 08:59:00', 1, 0)
#train_trans['dum_2'] = np.where(train_trans.index == '2020-05-01 20:59:00', 1, 0)
#train_trans['dum_3'] = np.where(train_trans.index == '2020-05-20 17:59:00', 1, 0)
#train_trans['dum_4'] = np.where(train_trans.index == '2020-05-21 07:29:00', 1, 0)
#train_trans['dum_5'] = np.where(train_trans.index == '2020-05-19 08:59:00', 1, 0)
#print(train_trans.sample(5)); print()

# sanity check
#print(train_trans.dum_1.unique())
#print(train_trans.dum_2.unique())

"""## 2.2 Granger causality test:"""

print('GBPUSD-> EURUSD')
ca_1 = model_l_2.test_causality(causing=['gbpusd'], caused=['eurusd'], kind='wald',signif=0.05 )
print(ca_1)
print("**********************")
print('GBPUSD-> USDJPY')
ca_2 = model_l_2.test_causality(causing=['gbpusd'], caused=['usdjpy'], kind='wald',signif=0.05 )
print(ca_2)
print("**********************")
print('GBPUSD -> USDJPY, EURUSD')
ca_3 = model_l_2.test_causality(causing=['gbpusd'], caused=['usdjpy', 'eurusd'], kind='wald',signif=0.05 )
print(ca_3)
print("**********************")
print('EURUSD -> USDJPY')
ca_4 = model_l_2.test_causality(causing=['eurusd'], caused=['usdjpy'], kind='wald',signif=0.05 )
print(ca_4)
print("**********************")
print('EURUSD -> GBPUSD, USDJPY')
ca_5 = model_l_2.test_causality(causing=['eurusd'], caused=['gbpusd', 'usdjpy'], kind='wald',signif=0.05 )
print(ca_4)
print("**********************")

# fitting VAR model with associated freq & lags
#model_l_2_new = VAR(endog = train_trans).fit(maxlags = 2)

"""Let‚Äôs proceed with the forecast.

### 2.3.1 Forecasting:
We used 2 as the optimal order in fitting the VAR model. Thus, we will take the final 2 steps in the training data for forecasting the immediate next step (i.e., the first minute of the test data).
"""

train_trans.values[-2:]

"""Now, after fitting the model, we forecast for the test data where the last 2 minutes of training data set as lagged values and steps set as 10 minutes as we want to forecast for the next 10 minutes."""

laged_values = train_trans.values[-2:]
forecast = DataFrame(model_l_2.forecast(y= laged_values, steps=10), 
                     index = test_var.index, columns= test_var.columns + '_trans')
forecast

def inverse_diff(train_trans, forecast):
    """Revert back the differencing"""
    df_fc = forecast.copy()
    columns = test_var.columns
    for col in columns:        
        # Roll back 1st Diff
        df_fc[str(col)+'_fcast'] = train_log[col].iloc[-1] + df_fc[str(col)+'_trans'].cumsum()
    return df_fc

results = inverse_diff(train_trans, forecast)        
z = results.loc[:, ['eurusd_fcast', 'gbpusd_fcast', 
                       'gc_fcast','nq_fcast', 'usdjpy_fcast', 'es_fcast']]
# Roll back log transformation
pd.options.display.float_format = "{:.2f}".format
# z = math.exp(z)
z = np.exp(z) 
# numpy exponential function on multidimensinal array; e= 2.718281(approx)
print(z)

# creating actual vs predicted results table
z.index = test_var.index
lag2 = concat([test_var, z], axis=1)
lag2 = lag2[['eurusd','eurusd_fcast','gbpusd','gbpusd_fcast','usdjpy','usdjpy_fcast',
                 'gc','gc_fcast','nq','nq_fcast']]
lag2 = lag2.sort_values(by='timestamp', ascending=False)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
lag2.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast','gbpusd_fcast','usdjpy_fcast',
                                                               'nq_fcast','gc_fcast']])

"""***Other way of forecasting***"""

pd.options.display.float_format = "{:.2f}".format
# Get the lag order
lag_order = model_l_2.k_ar
print(lag_order) 

# Input data for forecasting
input_data = train_trans.values[-lag_order:]
print(input_data)

# forecast
fc = model_l_2.forecast(y = input_data, steps = lag_order)
df_pred = DataFrame(fc, columns = train_trans.columns + '_1d')

"""
We are storing the value in numpy multidimensional array as a 64 bit floating point number.
The smallest non-zero number we can store is 2^(1-1023) = 2^-1022.
Since our obtained numbers are smaller than that, it is stored as 0.0 as shown below:
"""
print(df_pred)

# dropping dummy columns
#df_pred = df_pred.drop(columns= ['dum_1_1d', 'dum_2_1d', 'dum_3_1d', 'dum_4_1d', 'dum_5_1d'], axis=1)

# dropping ES column from train/test
train_var = train_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]
test_var = test_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]

"""### 2.3.2 Inverse transformation:"""

def inverse_diff(train_trans, df_pred):
    """Revert back the differencing"""
    df_fc = df_pred.copy()
    columns = train_trans.columns
    for col in columns:        
        # Roll back 1st Diff
        df_fc[str(col)+'_fcast'] = train_log[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = inverse_diff(train_var, df_pred)        
z = df_results.loc[:, ['eurusd_fcast', 'gbpusd_fcast', 
                       'gc_fcast','nq_fcast', 'usdjpy_fcast']]
print('Roll back 1st difference:')
print(z)

# Roll back log transformation
pd.options.display.float_format = "{:.2f}".format

# z = math.exp(z)
z = np.exp(z) 
# numpy exponential function on multidimensinal array; e= 2.718281(approx)
print(z)

#test_var.index[:2]

# creating actual vs predicted results table
z.index = test_var.index[:2]
lag2 = pd.concat([test_var[:2], z], axis=1)
lag2 = lag2[['eurusd','eurusd_fcast','gbpusd','gbpusd_fcast','usdjpy','usdjpy_fcast',
                 'gc','gc_fcast','nq','nq_fcast']]
lag2 = lag2.sort_values(by='timestamp', ascending=False)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
lag2.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast','gbpusd_fcast','usdjpy_fcast',
                                                               'nq_fcast','gc_fcast']])

#def inverse_diff(actual_df, pred_df):
  #df_res = pred_df.copy()
  #columns = actual_df.columns
  #for col in columns:
    #df_res[str(col)+'_pred'] = actual_
    #df[col].iloc[-1] + df_res[str(col)].cumsum()
  #return df_res

#res = inverse_diff(df[['es','eurusd','gbpusd','gc','nq', 'usdjpy']], df_pred)
#res

"""### 2.3.3 Accuracy Metrics: VAR Lag 2"""

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_error, mean_squared_log_error

# score EURUSD
mae = mean_absolute_error(lag2.eurusd, lag2.eurusd_fcast)
mse = mean_squared_error(lag2.eurusd, lag2.eurusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag2['eurusd'].values - lag2['eurusd_fcast'].values) / lag2['eurusd'].values)) * 100
msle = mean_squared_log_error(lag2.eurusd, lag2.eurusd_fcast)
sum = DataFrame(index = ['Mean Absolute Error', 'Mean squared error', 'Root mean squared error',
                            'Mean absolute % error','Mean squared log Error'])
sum['Accuracy metrics :    EURUSD'] = [mae, mse, rmse, mape, msle]

# score GBPUSD
mae = mean_absolute_error(lag2.gbpusd, lag2.gbpusd_fcast)
mse = mean_squared_error(lag2.gbpusd, lag2.gbpusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag2['gbpusd'].values - lag2['gbpusd_fcast'].values) / lag2['gbpusd'].values)) * 100
msle = mean_squared_log_error(lag2.gbpusd, lag2.gbpusd_fcast)
sum['GBPUSD'] = [mae, mse, rmse, mape, msle]

# score USDJPY
mae = mean_absolute_error(lag2.usdjpy, lag2.usdjpy_fcast)
mse = mean_squared_error(lag2.usdjpy, lag2.usdjpy_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag2['usdjpy'].values - lag2['usdjpy_fcast'].values) / lag2['usdjpy'].values)) * 100
msle = mean_squared_log_error(lag2.usdjpy, lag2.usdjpy_fcast)
sum['USDJPY'] = [mae, mse, rmse, mape, msle]

# score NQ
mae = mean_absolute_error(lag2.nq, lag2.nq_fcast)
mse = mean_squared_error(lag2.nq, lag2.nq_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag2['nq'].values - lag2['nq_fcast'].values) / lag2['nq'].values)) * 100
msle = mean_squared_log_error(lag2.nq, lag2.nq_fcast)
sum['NQ'] = [mae, mse, rmse, mape, msle]

# score GC
mae = mean_absolute_error(lag2.gc, lag2.gc_fcast)
mse = mean_squared_error(lag2.gc, lag2.gc_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag2['gc'].values - lag2['gc_fcast'].values) / lag2['gc'].values)) * 100
msle = mean_squared_log_error(lag2.gc, lag2.gc_fcast)
sum['GC'] = [mae, mse, rmse, mape, msle]
sum

"""#### 2.3.3.1 Forcast validation"""

# EURUSD forcast validation
m = lag2.loc[:, ['eurusd', 'eurusd_fcast']]
m['accuracy'] = round(m.apply(lambda row: row.eurusd_fcast / 
                                            row.eurusd *100, axis = 1),2)
m['accuracy'] = pd.Series(["{0:.2f}%".format(val) for val in m['accuracy']], 
                                     index = m.index)
m = m.round(decimals=3)
m = m.head(10)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
m.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast']])

#error_eurusd = (lag2.eurusd - lag2.eurusd_fcast)
# plot residuals
#error_eurusd.plot()
#plt.title('error plot :: EURUSD')
#plt.show()
#print()
#print('Mean of error:', error_eurusd.mean())

"""Running the example shows a seemingly random plot of the residual time series.

- The error is the difference between the observed data of the outcome variable ùë¶ and the predicted values ùë¶ÃÇ error = ùë¶ ‚àí ùë¶ÃÇ
- The residuals plot should look ‚Äúrandom‚Äù (no discernible pattern)
- if the residuals are not random, they suggest that our model is systematically incorrect, meaning it can be improved

Mean error value 57 (not close enough to zero). It suggests that there may be some bias and that we may be able to further improve the model by performing a bias correction. This could be done by adding the mean residual error (57.68)
to forecasts.


"""

## histogram plot
#error_eurusd.hist()
#plt.show()
# density plot
#error_eurusd.plot(kind='kde')
#plt.show()

"""One assumption underlying linear regression is that the variance of the errors is normally distributed (follows a Gaussian distribution).

The distribution does have a Gaussian look, showing an exponential distribution with some asymmetry. This confirms that that assumptions made by the modeling process seems in the right direction.


"""

# Our expectation here is to have any correlation between the error.

#from pandas.plotting import autocorrelation_plot

#autocorrelation_plot(DataFrame(error_eurusd))
#plt.title('Correlation: Error plot')
#plt.show()

"""We see some significant autocorrelation trend across the plot.Significant autocorrelation in the error plot suggests that the model good at incorporating the relationship between observations and lagged observations."""

error_eurusd_lag2 = (lag2.eurusd - lag2.eurusd_fcast).mean()
print('error_eurusd (lag2):', error_eurusd_lag2)
error_gbpusd_lag2 = (lag2.gbpusd - lag2.gbpusd_fcast).mean()
print('error_gbpusd (lag2):', error_gbpusd_lag2)
error_usdjpy_lag2 = (lag2.usdjpy - lag2.usdjpy_fcast).mean()
print('error_usdjpy (lag2):', error_usdjpy_lag2)
error_gc_lag2 = (lag2.gc - lag2.gc_fcast).mean()
print('error_gc (lag2):', error_gc_lag2)
error_nq_lag2 = (lag2.nq - lag2.nq_fcast).mean()
print('error_nq (lag2):', error_nq_lag2)

"""We have identifiued the error values; though values are quite low, however, we can add these values with the respective forecasted values to reduce the error.

# **3 VAR model fitting with lag 11:**
"""

df = pd.read_csv("AL_final_data.csv")
df.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)
df = df.set_index('timestamp')
df.index = pd.to_datetime(df.index)

nobs=5
train_var, test_var = df[0:- nobs], df[- nobs:]

# Check size
#print(train_var.shape, test_var.shape); print()

# transfoming training data set
train_log = np.log(train_var) # log transformation
train_trans = train_log.diff().dropna() # 1st order differencing
#print('Description of diff data:'); print(train_trans.describe());

# droppping ES variable
train_trans.drop(columns = ['es'], axis=1, inplace=True)


import statsmodels.tsa.api as smt
from statsmodels.tsa.api import VAR

# pass "1min" frequency
train_trans.index = pd.DatetimeIndex(train_trans.index).to_period('1min')

# fitting VAR model with associated freq & lags
model_l_11 = VAR(endog = train_trans).fit(maxlags = 11) # BIC = 11

residuals = model_l_11.resid
print(residuals.shape)
residuals.index = (residuals.index).to_timestamp()

y_fitted = model_l_11.fittedvalues
plt.figure(figsize = (15,5))
plt.plot(residuals.values, label='resid')
plt.plot(y_fitted.values, label='linear prediction')
plt.xlabel('lag')
plt.xticks(rotation=45)
plt.ylabel('Residuals')
plt.title('Residuals plot = VAR lag 11')
plt.grid(True)
plt.legend()
plt.show()

# test for structural breaks
name = ['test statistic', 'pval', 'crit']
test = sms.breaks_cusumolsresid(residuals.values, ddof = model_l_11.df_model)
lzip(name, test)

# residuals statistic
residuals.describe()

from statsmodels.stats.stattools import durbin_watson
#print('Durbin Watson statistic:')
#out = durbin_watson(model_l_12.resid)

#for col, val in zip(train_var.columns, out):
    #print((col), ':', round(val, 2))
print()

# Forecasting
pd.options.display.float_format = "{:.2f}".format
# Get the lag order
lag_order = model_l_11.k_ar
#print(lag_order) 

# Input data for forecasting
input_data = train_trans.values[-lag_order:]
#print(input_data)

# Input data for forecasting
input_data = train_trans.values[-lag_order:]
#print(input_data)

fc = model_l_11.forecast(y = input_data, steps = nobs)
df_pred = DataFrame(fc, columns = train_trans.columns + '_1d')
#print(df_pred)


# dropping ES column from train/test
train_var = train_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]
test_var = test_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]

def inverse_diff(train_trans, df_pred):
    """Revert back the differencing"""
    df_fc = df_pred.copy()
    columns = train_trans.columns
    for col in columns:        
        # Roll back 1st Diff
        df_fc[str(col)+'_fcast'] = train_log[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = inverse_diff(train_var, df_pred)        
z = df_results.loc[:, ['eurusd_fcast', 'gbpusd_fcast', 
                       'gc_fcast','nq_fcast', 'usdjpy_fcast']]

# Roll back log transformation
pd.options.display.float_format = "{:.2f}".format
z = np.exp(z)
#print(z)

print('Actual vs forecast:')
z.index = test_var.index
lag11 = pd.concat([test_var, z], axis=1)
lag11 = lag11[['eurusd','eurusd_fcast','gbpusd','gbpusd_fcast','usdjpy','usdjpy_fcast',
                 'gc','gc_fcast','nq','nq_fcast']]
lag11 = lag11.sort_values(by='timestamp', ascending=False)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
lag11.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast','gbpusd_fcast','usdjpy_fcast',
                                                               'nq_fcast','gc_fcast']])

# EURUSD forcast validation
print('EURUSD forcast validation:')
m = lag11.loc[:, ['eurusd', 'eurusd_fcast']]
m['accuracy'] = round(m.apply(lambda row: row.eurusd_fcast / 
                                            row.eurusd *100, axis = 1),2)
m['accuracy'] = pd.Series(["{0:.2f}%".format(val) for val in m['accuracy']], 
                                     index = m.index)
m = m.round(decimals=3)
#m = m.head(10)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
m.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast']])

# Accuracy metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_error, mean_squared_log_error

# score EURUSD
mae = mean_absolute_error(lag11.eurusd, lag11.eurusd_fcast)
mse = mean_squared_error(lag11.eurusd, lag11.eurusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag11['eurusd'].values - lag11['eurusd_fcast'].values) / lag11['eurusd'].values)) * 100
msle = mean_squared_log_error(lag11.eurusd, lag11.eurusd_fcast)
sum = DataFrame(index = ['Mean Absolute Error', 'Mean squared error', 'Root mean squared error',
                            'Mean absolute % error','Mean squared log Error'])
sum['Accuracy metrics :    EURUSD'] = [mae, mse, rmse, mape, msle]

# score GBPUSD
mae = mean_absolute_error(lag11.gbpusd, lag11.gbpusd_fcast)
mse = mean_squared_error(lag11.gbpusd, lag11.gbpusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag11['gbpusd'].values - lag11['gbpusd_fcast'].values) / lag11['gbpusd'].values)) * 100
msle = mean_squared_log_error(lag11.gbpusd, lag11.gbpusd_fcast)
sum['GBPUSD'] = [mae, mse, rmse, mape, msle]

# score USDJPY
mae = mean_absolute_error(lag11.usdjpy, lag11.usdjpy_fcast)
mse = mean_squared_error(lag11.usdjpy, lag11.usdjpy_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag11['usdjpy'].values - lag11['usdjpy_fcast'].values) / lag11['usdjpy'].values)) * 100
msle = mean_squared_log_error(lag11.usdjpy, lag11.usdjpy_fcast)
sum['USDJPY'] = [mae, mse, rmse, mape, msle]

# score NQ
mae = mean_absolute_error(lag11.nq, lag11.nq_fcast)
mse = mean_squared_error(lag11.nq, lag11.nq_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag11['nq'].values - lag11['nq_fcast'].values) / lag11['nq'].values)) * 100
msle = mean_squared_log_error(lag11.nq, lag11.nq_fcast)
sum['NQ'] = [mae, mse, rmse, mape, msle]

# score GC
mae = mean_absolute_error(lag11.gc, lag11.gc_fcast)
mse = mean_squared_error(lag11.gc, lag11.gc_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag11['gc'].values - lag11['gc_fcast'].values) / lag11['gc'].values)) * 100
msle = mean_squared_log_error(lag11.gc, lag11.gc_fcast)
sum['GC'] = [mae, mse, rmse, mape, msle]
print('Accuracy metrics:')
print(sum); print()

print('error to be added with forecasted values:'); print()
error_eurusd_lag11 = (lag11.eurusd - lag11.eurusd_fcast).mean()
print('error_eurusd (lag11):', error_eurusd_lag11)
error_gbpusd_lag11 = (lag11.gbpusd - lag11.gbpusd_fcast).mean()
print('error_gbpusd (lag11):', error_gbpusd_lag11)
error_usdjpy_lag11 = (lag11.usdjpy - lag11.usdjpy_fcast).mean()
print('error_usdjpy (lag11):', error_usdjpy_lag11)
error_gc_lag11 = (lag11.gc - lag11.gc_fcast).mean()
print('error_gc (lag11):', error_gc_lag11)
error_nq_lag11 = (lag11.nq - lag11.nq_fcast).mean()
print('error_nq (lag11):', error_nq_lag11)

"""# **5 VAR model fitting with lag 31:**"""

df = pd.read_csv("AL_final_data.csv")
df.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)
df = df.set_index('timestamp')
df.index = pd.to_datetime(df.index)

# spliting train/test
nobs = 5 # keeping 10 mins observations aside for validation
train_var, test_var = df[0:- nobs], df[- nobs:]

# Check size
#print(train_var.shape); print(test_var.shape);

# transfoming training data set
train_log = np.log(train_var) # log transformation
train_trans = train_log.diff().dropna() # 1st order differencing
#print('Description of diff data:'); print(train_trans.describe())

# droppping ES variable
train_trans.drop(columns = ['es'], axis=1, inplace=True)


import statsmodels.tsa.api as smt
from statsmodels.tsa.api import VAR

# pass "1min" frequency
train_trans.index = pd.DatetimeIndex(train_trans.index).to_period('1min')

# fitting VAR model with associated freq & lags
model_l_31 = VAR(endog = train_trans).fit(maxlags = 31)

residuals = model_l_31.resid
print(residuals.shape)
residuals.index = (residuals.index).to_timestamp()

y_fitted = model_l_31.fittedvalues
plt.figure(figsize = (15,5))
plt.plot(residuals.values, label='resid')
plt.plot(y_fitted.values, label='linear prediction')
plt.xlabel('lag')
plt.xticks(rotation=45)
plt.ylabel('Residuals')
plt.title('Residuals plot = VAR lag 31')
plt.grid(True)
plt.legend()
plt.show()

# test for structural breaks
name = ['test statistic', 'pval', 'crit']
test = sms.breaks_cusumolsresid(residuals.values, ddof = model_l_31.df_model)
lzip(name, test)

# residuals statistic
residuals.describe()

# Forecasting
pd.options.display.float_format = "{:.2f}".format
# Get the lag order
lag_order = model_l_31.k_ar
#print(lag_order) 

# Input data for forecasting
input_data = train_trans.values[-lag_order:]
#print(input_data)

# Input data for forecasting
input_data = train_trans.values[-lag_order:]
#print(input_data)

fc = model_l_31.forecast(y = input_data, steps = nobs)
df_pred = DataFrame(fc, columns = train_trans.columns + '_1d')
#print(df_pred)


# dropping ES column from train/test
train_var = train_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]
test_var = test_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]

# dropping ES column from train/test
train_var = train_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]
test_var = test_var[['eurusd', 'gbpusd','usdjpy','nq','gc']]

def inverse_diff(train_trans, df_pred):
    """Revert back the differencing"""
    df_fc = df_pred.copy()
    columns = train_trans.columns
    for col in columns:        
        # Roll back 1st Diff
        df_fc[str(col)+'_fcast'] = train_log[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = inverse_diff(train_var, df_pred)        
z = df_results.loc[:, ['eurusd_fcast', 'gbpusd_fcast', 
                       'gc_fcast','nq_fcast', 'usdjpy_fcast']]

# Roll back log transformation
pd.options.display.float_format = "{:.2f}".format
z = np.exp(z)

print('Actual vs forecast:')
z.index = test_var.index
lag31 = concat([test_var, z], axis=1)
lag31 = lag31[['eurusd','eurusd_fcast','gbpusd','gbpusd_fcast','usdjpy','usdjpy_fcast',
                 'gc','gc_fcast','nq','nq_fcast']]
lag31 = lag31.sort_values(by='timestamp', ascending=False)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
lag31.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast','gbpusd_fcast','usdjpy_fcast',
                                                               'nq_fcast','gc_fcast']])

# EURUSD forcast validation
print('EURUSD forcast validation:')
m = lag31.loc[:, ['eurusd', 'eurusd_fcast']]
m['accuracy'] = round(m.apply(lambda row: row.eurusd_fcast / 
                                            row.eurusd *100, axis = 1),2)
m['accuracy'] = pd.Series(["{0:.2f}%".format(val) for val in m['accuracy']], 
                                     index = m.index)
m = m.round(decimals=3)
#m = m.head(10)
def highlight_cols(s):
    color = 'yellow'
    return 'background-color: %s' % color
m.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_fcast']])

# Accuracy metrics
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_error, mean_squared_log_error

# score EURUSD
mae = mean_absolute_error(lag31.eurusd, lag31.eurusd_fcast)
mse = mean_squared_error(lag31.eurusd, lag31.eurusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag31['eurusd'].values - lag31['eurusd_fcast'].values) / lag31['eurusd'].values)) * 100
msle = mean_squared_log_error(lag31.eurusd, lag31.eurusd_fcast)
sum = DataFrame(index = ['Mean Absolute Error', 'Mean squared error', 'Root mean squared error',
                            'Mean absolute % error','Mean squared log Error'])
sum['Accuracy metrics :    EURUSD'] = [mae, mse, rmse, mape, msle]

# score GBPUSD
mae = mean_absolute_error(lag31.gbpusd, lag31.gbpusd_fcast)
mse = mean_squared_error(lag31.gbpusd, lag31.gbpusd_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag31['gbpusd'].values - lag31['gbpusd_fcast'].values) / lag31['gbpusd'].values)) * 100
msle = mean_squared_log_error(lag31.gbpusd, lag31.gbpusd_fcast)
sum['GBPUSD'] = [mae, mse, rmse, mape, msle]

# score USDJPY
mae = mean_absolute_error(lag31.usdjpy, lag31.usdjpy_fcast)
mse = mean_squared_error(lag31.usdjpy, lag31.usdjpy_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag31['usdjpy'].values - lag31['usdjpy_fcast'].values) / lag31['usdjpy'].values)) * 100
msle = mean_squared_log_error(lag31.usdjpy, lag31.usdjpy_fcast)
sum['USDJPY'] = [mae, mse, rmse, mape, msle]

# score NQ
mae = mean_absolute_error(lag31.nq, lag31.nq_fcast)
mse = mean_squared_error(lag31.nq, lag31.nq_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag31['nq'].values - lag31['nq_fcast'].values) / lag31['nq'].values)) * 100
msle = mean_squared_log_error(lag31.nq, lag31.nq_fcast)
sum['NQ'] = [mae, mse, rmse, mape, msle]

# score GC
mae = mean_absolute_error(lag31.gc, lag31.gc_fcast)
mse = mean_squared_error(lag31.gc, lag31.gc_fcast)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((lag31['gc'].values - lag31['gc_fcast'].values) / lag31['gc'].values)) * 100
msle = mean_squared_log_error(lag31.gc, lag31.gc_fcast)
sum['GC'] = [mae, mse, rmse, mape, msle]
print('Accuracy metrics:')
print(sum); print()

print('error to be added with forecasted values:'); print()
error_eurusd_lag31 = (lag31.eurusd - lag31.eurusd_fcast).mean()
print('error_eurusd (lag31):', error_eurusd_lag31)
error_gbpusd_lag31 = (lag31.gbpusd - lag31.gbpusd_fcast).mean()
print('error_gbpusd (lag31):', error_gbpusd_lag31)
error_usdjpy_lag31 = (lag31.usdjpy - lag31.usdjpy_fcast).mean()
print('error_usdjpy (lag31):', error_usdjpy_lag31)
error_gc_lag31 = (lag31.gc - lag31.gc_fcast).mean()
print('error_gc (lag31):', error_gc_lag31)
error_nq_lag31 = (lag31.nq - lag31.nq_fcast).mean()
print('error_nq (lag31):', error_nq_lag31)

"""# **6 Out of sample forecast:**

"""

print('.......load necessary libraries......')
!pip install pyforest
from pyforest import *
import datetime, pickle, copy, warnings
from pandas import DataFrame, merge, concat
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
from termcolor import colored
import statsmodels.tsa.api as smt
from statsmodels.tsa.api import VAR
import sklearn.externals
import joblib
import math

print()
print(colored('OUT OF SAMPLE FUTURE PREDICTION:', 'blue', attrs=['bold']))
df = pd.read_csv("AL_final_data.csv")
df.drop(columns = ['Unnamed: 0', 'es'], axis=1, inplace=True)
df = df.set_index('timestamp')
df.index = pd.to_datetime(df.index)

# transfoming training data set
df_log = np.log(df) # log transformation
df_trans = df_log.diff().dropna() # 1st order differencing
#print('Description of diff data:'); print(df_trans.describe());

# pass "1min" frequency
df_trans.index = pd.DatetimeIndex(df_trans.index).to_period('1min')

max_lags = int(2)
# fitting VAR model with associated freq & lags
model_oos = VAR(endog = df_trans).fit(maxlags = max_lags)

# Save model to file in the current working directory
joblib_file = "joblib_ALVAR_model.pkl"  
joblib.dump(model_oos, joblib_file)

# Load from file
joblib_ALVAR_model = joblib.load(joblib_file)

# Forecasting
pd.options.display.float_format = "{:.2f}".format
# Get the lag order
lag_order = joblib_ALVAR_model.k_ar # lag order =2
#print(lag_order) 

# Input data for forecasting
input_data = df_trans.values[-lag_order:]
#print(input_data)

# Input data for forecasting
input_data = df_trans.values[-lag_order:]
#print(input_data)

fc = joblib_ALVAR_model.forecast(y = input_data, steps = max_lags)
df_pred = DataFrame(fc, columns = df_trans.columns + '_1d')
#print(df_pred)

def inverse_diff(df_trans, df_pred):
    """Revert back the differencing"""
    df_fc = df_pred.copy()
    columns = df_trans.columns
    for col in columns:        
        # Roll back 1st Diff
        df_fc[str(col)+'_fcast'] = df_log[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = inverse_diff(df_trans, df_pred)        
z = df_results.loc[:, ['eurusd_fcast', 'gbpusd_fcast', 
                       'gc_fcast','nq_fcast', 'usdjpy_fcast']]

# Roll back log transformation
pd.options.display.float_format = "{:.2f}".format
z = np.exp(z) 
# numpy exponential function on our multidimensional array; e= 2.718281(approx)

"""
we have 2 future values now; we have to index these values with future timestamp
"""

d = df.tail(max_lags)
d.reset_index(inplace = True)
d = d.append(DataFrame({'timestamp': pd.date_range(start = d.timestamp.iloc[-1], 
                                             periods = (len(d)+1), freq = '1min', closed = 'right')}))
d.set_index('timestamp', inplace = True)
d = d.tail(max_lags)
z.index = d.index
print(colored('THESE ARE NEXT 2 MINUTES PREDICTION:', 'red', attrs=['bold']))
#print(z); print()

# Adding training error to reduce prediction error:
#print('EURUSD forecast:')
eurusd_fcast = z.eurusd_fcast + error_eurusd_lag2
#print(eurusd_fcast); print()

#print('GBPUSD forecast:')
gbpusd_fcast = z.gbpusd_fcast + error_gbpusd_lag2
#print(gbpusd_fcast); print()

#print('USDJPY forecast:')
usdjpy_fcast = z.usdjpy_fcast + error_gbpusd_lag2
#print(usdjpy_fcast); print()

#print('GC forecast:')
gc_fcast = z.gc_fcast + error_gc_lag2
#print(gc_fcast); print()

#print('NQ forecast:')
nq_fcast = z.nq_fcast + error_nq_lag2
#print(nq_fcast); print()

concat([eurusd_fcast, gbpusd_fcast, usdjpy_fcast, nq_fcast, gc_fcast], axis=1)