# -*- coding: utf-8 -*-
"""correlation_23Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNM4I__Z0VGulWRd4ieic8ZvD0wD6gIJ
"""

!pip install pyforest
from pyforest import *
import datetime, pickle, copy, warnings
from pandas import DataFrame, merge, concat
import glob
plt.style.use('dark_background')
from google.colab import files

uploaded = files.upload()

print('view data...')
df = pd.read_csv("AL_23_series.csv")
print('dropping unnecessary column...')
df.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)
print('setting datetime index....')
df = df.set_index('timestamp')
df.index = pd.to_datetime(df.index)
print('dropping duplicates (if any)...')
df.drop_duplicates(keep=False)
df.sample(5) # random samples

# sorting rows in ascending order
df = df.sort_values(by='timestamp', ascending=True)

"""## 1 Visualization:"""

# visualizing 10 series
x = df[['6e','6j','es','eurusd','gbpusd','gc','nq','usdjpy','audjpy']]

# Plot
plt.style.use('dark_background')
fig, axes = plt.subplots(nrows=3, ncols=3, dpi=120, figsize=(15,6))

for i, ax in enumerate(axes.flatten()):
    dataset = x[x.columns[i]]
    ax.plot(dataset, color='red', linewidth=1)
    # Decorations
    ax.set_title(x.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines['top'].set_alpha(0)
    ax.tick_params(labelsize=6)
    plt.tight_layout();

# visualizing 10 series
x = df[['audnzd','audusd','euraud','eurchf','si','nzdusd','tnm0','ub','usdcad']]

# Plot
plt.style.use('dark_background')
fig, axes = plt.subplots(nrows=3, ncols=3, dpi=120, figsize=(15,6))

for i, ax in enumerate(axes.flatten()):
    dataset = x[x.columns[i]]
    ax.plot(dataset, color='red', linewidth=1)
    # Decorations
    ax.set_title(x.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines['top'].set_alpha(0)
    ax.tick_params(labelsize=6)
    plt.tight_layout();

# visualizing 10 series
x = df[['ymm0','zbm0','zfm0','znm0']]

# Plot
plt.style.use('dark_background')
fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(15,6))

for i, ax in enumerate(axes.flatten()):
    dataset = x[x.columns[i]]
    ax.plot(dataset, color='red', linewidth=1)
    # Decorations
    ax.set_title(x.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines['top'].set_alpha(0)
    ax.tick_params(labelsize=6)
    plt.tight_layout();

# visualizing 10 series
plt.figure(figsize= (10,5))
plt.plot(df['ztm0'], color ='r')
plt.title('ztm0'); plt.show()

df.aggregate([min, np.mean,np.std,np.median,max]).round(2).transpose()

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
import matplotlib.pyplot as plt
from sklearn.feature_selection import mutual_info_regression

"""## 2 Unit Root:
Agumented Dickey-Fuller test for stationarity to look for unit roots by attempting to fit an autoregressive model to the data.

### 2.1 Augmented Dickey-Fuller
- H0 this test is that there is a unit root. 
- H1 is that the time series is stationary, or trend stationary. 

If the test statistic < critical Value, we can reject the null hypothesis and say that the series is stationary; if the p-value is less than alpha we can reject the null hypothesis.
"""

from statsmodels.tsa.stattools import adfuller

def adfuller_test(series, signif=0.05, name='', verbose=False):
    """Perform ADFuller to test for Stationarity of given series and print report"""
    r = adfuller(series, autolag='AIC')
    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
    p_value = output['pvalue'] 
    def adjust(val, length= 6): return str(val).ljust(length)

    # Print Summary
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[4].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Non-Stationary.")
        
# ADF Test on each column
for name, column in df.iteritems():
    adfuller_test(column, name=column.name)
    print()

"""### 2.2 KPSS:

Reject the null hypothesis of stationarity if the value of the test statistic > 10%, 5% and 1% critical values. 
- The null hypothesis is that the data is stationary, 
- the alternate hypothesis for the test is that the data is not stationary.

If we fail to reject the null hypothesis it means our time series is stationary or trend stationary, because KPSS classifies a series as stationary on the absence of a unit root.
"""

from statsmodels.tsa.stattools import kpss
import warnings
warnings.filterwarnings("ignore")

from statsmodels.tsa.stattools import kpss
def kpss_test(timeseries, signif=0.05, name='', verbose=False):
    print ('Results of KPSS Test:')
    r = kpss(timeseries, regression='c', lags="auto")
    output = {'test_statistic':round(r[0], 3), 'pvalue':round(r[1], 3), 'n_lags':round(r[2], 3), 'n_obs':r[3]}
    p_value = output['pvalue']
    def adjust(val, length= 6): return str(val).ljust(length)

  
# Print Summary
    print(f'    KPSS Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[3].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is: Non-Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Stationary.")

# KPSS Test on each column
for name, column in df.iteritems():
    kpss_test(column, name = column.name)
    print()

"""
test statistics with non-standard distributions is that the available p-values are available only for a restricted range. 
If the value is outside this range, then only the boundary value is reported and the actual p-value is somewhere above the
upper limit or somewhere below the lower limit.
If reported p-value is at the upper bound 0.1, the actual p-value will be larger than 0.1, 
so we cannot reject the Null hypothesis of stationarity. 
If the reported p-value is the lower bound of the range 0.01, and the actual p-value will be smaller than 0.01. 
So we can reject the Null hypothesis of stationarity with alpha 0.01.
"""

"""# 3 Changes over time"""

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)

# spliting train/test
nobs = 10 # keeping 10 mins observations aside for validation
train, test = df[0:- nobs], df[- nobs:]

# Check size
print(train.shape, test.shape); print()

"""
stationary time series means a constant mean and variance over time, which makes it easy for predicting values.
From a statistics point of view, absolute changes are not optimal because they
are dependent on the scale of the time series data itself. Therefore, % changes are usually preferred.
However, I have used logdiff whic is an alternative to percentage changewhic are easier to handle.
"""

# log transfoming training data set
train_log = np.log(train) # log transformation
train_trans = train_log.diff().dropna() # 1st order differencing
# differenced data will contain one less point than the original data.
print('Description of diff data:'); print(train_trans.describe().transpose());

"""LogDiff normalizing the data as well; by using normalization methods it is possible to significantly reduce correlation. Normalization procedures affect both the true correlation, stemming from features interactions, and the spurious correlation induced by random noise."""

train_trans.hist(figsize=(15, 6),color='w',bins=100)
plt.tight_layout()

"""## 3.1 Unit root test (transformed data):"""

from statsmodels.tsa.stattools import adfuller

def adfuller_test(series, signif=0.05, name='', verbose=False):
    """Perform ADFuller to test for Stationarity of given series and print report"""
    r = adfuller(series, autolag='AIC')
    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
    p_value = output['pvalue'] 
    def adjust(val, length= 6): return str(val).ljust(length)

    # Print Summary
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[4].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Non-Stationary.")
        
# ADF Test on each column
for name, column in train_trans.iteritems():
    adfuller_test(column, name=column.name)
    print()

from statsmodels.tsa.stattools import kpss
import warnings
warnings.filterwarnings("ignore")

from statsmodels.tsa.stattools import kpss
def kpss_test(timeseries, signif=0.05, name='', verbose=False):
    print ('Results of KPSS Test:')
    r = kpss(timeseries, regression='c', lags="auto")
    output = {'test_statistic':round(r[0], 3), 'pvalue':round(r[1], 3), 'n_lags':round(r[2], 3), 'n_obs':r[3]}
    p_value = output['pvalue']
    def adjust(val, length= 6): return str(val).ljust(length)

  
# Print Summary
    print(f'    KPSS Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[3].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is: Non-Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Stationary.")

# KPSS Test on each column
for name, column in train_trans.iteritems():
    kpss_test(column, name = column.name)
    print()

# saving final data set
train_trans.to_csv("AL_23_series_normalized.csv")

files.download('AL_23_series_normalized.csv')

"""# 4 Multi-colinearity:

## 4.1 Pearson correlation:
"""

# rechecking correlation on transformed data
col = train_trans.copy()
print('correlation matrix (differenced data):')
print(col.corr())
pearsoncorr = col.corr(method='pearson')

"""
Pearson correlation matrix and plots show the bivariate relationship between the independent variables.
"""

# Set up the matplotlib figure
plt.figure(figsize=(20,10))
sns.heatmap(pearsoncorr, xticklabels=pearsoncorr.columns,
            yticklabels=pearsoncorr.columns,
            cmap='RdBu_r',annot=True,linewidth=0.5,
            vmax=.3, center=0)
plt.show()

"""1.  nq & es (0,91)
2.  si & gc (0,79)
3.  zbm0 & tnm0 (0,90)
4.  zbm0 & ub (0.80)
5.  zfm0 & tnm0 (0,68)
6.  zfm0 & ub (0.59)
7.  zmno & tnmo (0,63)
8.  znm0 & ub (0.58)

## 4.2 VIF:
R^2 value shows how well an independent variable is described by the other independent variables. 

High value of R^2 means that the variable is highly correlated with the other variables. 

Closer the R^2 value to 1, the higher the value of VIF and the higher the multicollinearity with the particular independent variable.

VIF = 1 / (1 - R^2)
"""

# Import library for VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF
    vif = DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

"""
VIF starts at 1 and has no upper limit
VIF = 1, no correlation between the independent variable and the other variables
VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others
"""

calc_vif(train_trans)

"""Here, we need to drop ES, NQ, TNM0, ZBM0 which have values > 5.

We shall drop one at a time and check the pearson corralrion and VIF.
"""